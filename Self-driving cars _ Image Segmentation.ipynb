{"cells":[{"metadata":{},"cell_type":"markdown","source":"Basically, segmentation is a process that partitions an image into regions. It is an image processing approach that allows us to separate objects and textures in images. Segmentation is especially preferred in applications such as remote sensing or tumor detection in biomedicine."},{"metadata":{},"cell_type":"markdown","source":"U-Net is more successful than conventional models, in terms of architecture and in terms pixel-based image segmentation formed from convolutional neural network layers. It’s even effective with limited dataset images. The presentation of this architecture was first realized through the analysis of biomedical images."},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport os\nfrom skimage.io import imread as imread\nfrom PIL import Image\nimport imageio\nimport random\n\nimport tensorflow as tf\nfrom pylab import rcParams\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn import metrics\n\nfrom sklearn.model_selection import GridSearchCV\n\nfrom keras.models import Model, load_model\nfrom keras import optimizers, losses, activations, models\n\nfrom keras.layers import Input, Dropout, concatenate, GlobalAveragePooling2D\nfrom keras.layers import UpSampling2D\n#from keras.layers.core import Lambda, RepeatVector, Reshape\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers import Convolution2D, ZeroPadding2D, MaxPooling2D, Cropping2D, Conv2D\n\nfrom keras.layers.pooling import MaxPooling2D\n#from keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.callbacks import TensorBoard\n\nfrom keras import backend as K\nfrom keras.utils.np_utils import to_categorical\nfrom keras.preprocessing.image import ImageDataGenerator\n\n\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten, BatchNormalization\nfrom keras import applications\nfrom keras.applications import resnet50\nfrom keras.optimizers import Adam\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n#Set numpy and Tensorflow random seed to mask sure experiment reproducible(only works in CPU mode).\nfrom numpy.random import seed\nseed(123)\nfrom tensorflow import set_random_seed\nset_random_seed(123)\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport imageio\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"image_path = '../input/dataa/dataA/CameraRGB/'\nmask_path = '../input/dataa/dataA/CameraSeg/'\n\nimage_list = os.listdir(image_path)\nmask_list = os.listdir(mask_path)\nimage_list = [image_path+i for i in image_list]\nmask_list = [mask_path+i for i in mask_list]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ed43de07ac393b915b893b9f16222987be5ba74"},"cell_type":"code","source":"N = 1\nimg = imageio.imread(image_list[N])\nmask = imageio.imread(mask_list[N])\nmask = np.array([max(mask[i, j]) for i in range(mask.shape[0]) for j in range(mask.shape[1])]).reshape(img.shape[0], img.shape[1])\n\nfig, arr = plt.subplots(1, 2, figsize=(14, 10))\narr[0].imshow(img)\narr[0].set_title('Image')\narr[1].imshow(mask, cmap='Paired')\narr[1].set_title('Segmentation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mask.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N = 121\nimg = imageio.imread(image_list[N])\nmask = imageio.imread(mask_list[N])\nprint(mask.shape)\nmask = np.array([max(mask[i, j]) for i in range(mask.shape[0]) for j in range(mask.shape[1])]).reshape(img.shape[0], img.shape[1])\nprint(mask.shape)\n\nfig, arr = plt.subplots(1, 2, figsize=(14, 10))\narr[0].imshow(img)\narr[0].set_title('Image')\narr[1].imshow(mask, cmap='Paired')\narr[1].set_title('Segmentation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 13 Classes from 0 to 12\nset([z for i in mask for z in i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"codes = [\"Unlabeled\",\n          \"Building\",\n          \"Fence\",\n          \"Other\",\n          \"Pedestrian\",\n          \"Pole\",\n          \"Road line\",\n          \"Road\",\n          \"Sidewalk\",\n          \"Vegetation\",\n          \"Car\",\n          \"Wall\",\n          \"Traffic sign\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"codes[6]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(image_list), len(mask_list)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9917da8bc1a5930c04707bd7621e28fa4bf3721a"},"cell_type":"markdown","source":"## Road segmentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"road = np.zeros((600, 800))\nroad[np.where(mask==7)[0], np.where(mask==7)[1]]=1\nplt.imshow(road)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36e09e734196a3c84959b84a423d8fbf0434756f"},"cell_type":"code","source":"from tqdm import tqdm\nimport sys\nimport dask\nimport dask.dataframe as dd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len_images = len(image_list)\nIMG_CHANNELS = 3\nIMG_HEIGHT, IMG_WIDTH = 600, 800\n\n\nnumber = 300\n\nimages = np.zeros((number, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\nmasks = np.zeros((number, IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.uint8)\n\n\nfor n in range(number):\n\n    if image_list[n].split('.')[-1] == 'png':\n\n        #print(filename)\n        img = imageio.imread(image_list[n])\n        mask = imageio.imread(mask_list[n])\n        \n        mask_new = np.zeros((600, 800, 1), dtype=np.int8)\n        for i in range(13):\n            mask_new[np.where(mask==i)[0], np.where(mask==i)[1]]=i\n       \n        \n        images[n] = img\n        masks[n] = mask_new\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images.shape, masks.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set(masks[2].flatten())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split Train / val / test"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(123)\nshuffle_ids = np.array([i for i in range(len(masks))])\nnp.random.shuffle(shuffle_ids)\n\ntrain_ids = shuffle_ids[:int(len(masks)*0.8)]\nval_ids = shuffle_ids[int(len(masks)*0.8):int(len(masks)*0.8+30)]\ntest_ids = shuffle_ids[int(len(masks)*0.8+30):]\n\nX_train, train_masks = images[train_ids], masks[train_ids]\nX_val, val_masks = images[val_ids], masks[val_ids]\nX_test, test_masks = images[test_ids], masks[test_ids]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, X_val.shape, X_test.shape\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# To categorical"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_classes = 13\n\n#y_train_categorize = to_categorical(train_masks, num_classes)\n#y_val_categorize = to_categorical(val_masks, num_classes)\n#y_test_categorize = to_categorical(test_masks, num_classes)\n\n#train_masks.shape, y_train_categorize.shape\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check if training data looks all right\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#ix = random.randint(0, number)\n#plt.imshow(X_train[ix])\n#plt.show()\nplt.imshow(np.squeeze(train_masks[0]))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Image generator"},{"metadata":{},"cell_type":"markdown","source":"Un simple changement dans data augmentation change la val accuracy.\n* Shear range = 0.5\n* Zoom range = 0.3\n* Horizontal flip = True\n* Rescale = 1./255"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_datagen = ImageDataGenerator(\n        rescale=1./255,\n        shear_range=0.6,\n        zoom_range=0.6,\n        horizontal_flip=True)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_datagen = ImageDataGenerator(\n        rescale=1./255)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_datagen = ImageDataGenerator(\n        rescale=1./255)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#train_generator = train_datagen.flow(X_train, y_train_categorize, batch_size=5, shuffle=True)\ntrain_generator = train_datagen.flow(X_train, train_masks, batch_size=5, shuffle=True)\n\nSTEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#val_generator = val_datagen.flow(X_val, y_val_categorize, batch_size=5, shuffle=True)\nval_generator = val_datagen.flow(X_val, val_masks, batch_size=5, shuffle=True)\n\nSTEP_SIZE_VAL=val_generator.n//val_generator.batch_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_generator = test_datagen.flow(X_test, y_test_categorize, batch_size=1, shuffle=True)\ntest_generator = test_datagen.flow(X_test, test_masks, batch_size=1, shuffle=False)\n\nSTEP_SIZE_TEST=test_generator.n//test_generator.batch_size","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8aea1882bcf1cb8464b5ca04c7384c809d5b95b6"},"cell_type":"markdown","source":"## Build U-Net with subtle changes"},{"metadata":{"trusted":true,"_uuid":"3a98984ba2287d540c49d83ec73381de44daea05"},"cell_type":"code","source":"from keras.models import Model, load_model\nfrom keras.layers import Input\nfrom keras.layers.core import Lambda, RepeatVector, Reshape\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras import backend as K","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1ce0027b3c2af4fc889d37b07ecce9308b857f6"},"cell_type":"code","source":"# Build U-Net model\ninput_img = Input((IMG_HEIGHT, IMG_WIDTH, 3), name='img')\n\n\nc1 = Conv2D(8, (3, 3), activation='relu', padding='same') (input_img)\nc1 = Conv2D(8, (3, 3), activation='relu', padding='same') (c1)\np1 = MaxPooling2D((2, 2)) (c1)\n\nc2 = Conv2D(16, (3, 3), activation='relu', padding='same') (p1)\nc2 = Conv2D(16, (3, 3), activation='relu', padding='same') (c2)\np2 = MaxPooling2D((2, 2)) (c2)\n\nc3 = Conv2D(32, (3, 3), activation='relu', padding='same') (p2)\nc3 = Conv2D(32, (3, 3), activation='relu', padding='same') (c3)\np3 = MaxPooling2D((2, 2)) (c3)\n\nc4 = Conv2D(64, (3, 3), activation='relu', padding='same') (p3)\nc4 = Conv2D(64, (3, 3), activation='relu', padding='same') (c4)\n\nu5 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c4)\nu5 = concatenate([u5, c3])\nc6 = Conv2D(32, (3, 3), activation='relu', padding='same') (u5)\nc6 = Conv2D(32, (3, 3), activation='relu', padding='same') (c6)\n\nu7 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c6)\nu7 = concatenate([u7, c2])\nc7 = Conv2D(16, (3, 3), activation='relu', padding='same') (u7)\nc7 = Conv2D(16, (3, 3), activation='relu', padding='same') (c7)\n\nu8 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c7)\nu8 = concatenate([u8, c1])\nc8 = Conv2D(8, (3, 3), activation='relu', padding='same') (u8)\nc8 = Conv2D(8, (3, 3), activation='relu', padding='same') (c8)\n\noutputs = Conv2D(13, (1, 1), activation='sigmoid') (c8)\n\nmodel = Model(inputs=[input_img], outputs=[outputs])\n\n\n#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) #, metrics=[mean_iou]) # The mean_iou metrics seens to leak train and test values...\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) #, metrics=[mean_iou]) # The mean_iou metrics seens to leak train and test values...\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define callbacks settings"},{"metadata":{"trusted":true,"_uuid":"c65f6779623b0bbc5ebfaee723be6df05200c04e"},"cell_type":"code","source":"tb = TensorBoard(log_dir='logs', write_graph=True)\nmc = ModelCheckpoint(mode='max', filepath='camvid_model_150_epochs_checkpoint.h5', monitor='accuracy', save_best_only='True', save_weights_only='True', verbose=1)\nes = EarlyStopping(mode='max', monitor='val_accuracy', patience=10, verbose=1)\ncallbacks = [tb, mc, es]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train and save the U-Net model"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 5\nsteps_per_epoch = np.ceil(float(len(X_train) - round(0.1*len(X_train))) / float(batch_size))\nsteps_per_epoch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_steps = (float((round(0.1*len(X_train)))) / float(batch_size))\nvalidation_steps","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs = 200\n\nresult = model.fit_generator(train_generator, steps_per_epoch=18 ,\n                validation_data = val_generator, \n                validation_steps = validation_steps, epochs=num_epochs, callbacks=callbacks)\n\nmodel.save_weights(\"camvid_model_200_epochs.h5\", overwrite=True)\n\n#validation_steps=STEP_SIZE_VAL","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Learning curves"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1, figsize = (7,9)) \n    \nplt.subplot(211)  \nplt.plot(result.history['acc'])  \nplt.plot(result.history['val_acc'])  \nplt.title('model accuracy')  \nplt.ylabel('accuracy')  \nplt.xlabel('epoch')  \nplt.ylim(0,1)\nplt.legend(['train', 'valid']) \n#plt.grid(ls='--', c='C7')\n   \nplt.subplot(212)  \nplt.plot(result.history['loss'])  \nplt.plot(result.history['val_loss'])  \nplt.title('model loss')  \nplt.ylabel('loss')  \nplt.xlabel('epoch')  \nplt.legend(['train', 'valid']) \n#plt.grid(ls='--', c='C7')\n\nplt.savefig('/kaggle/working/learning_curves_unet.png', bbox_inches='tight')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reloading best weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load weights\nmodel.load_weights(\"/kaggle/working/camvid_model_200_epochs.h5\")\n# Compile model (required to make predictions)\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) #, metrics=[mean_iou]) # The mean_iou metrics seens to leak train and test values...\nprint(\"Created model and loaded weights from file\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction"},{"metadata":{},"cell_type":"markdown","source":" For each pixel the model predicts log probabilities for all classes."},{"metadata":{"trusted":true},"cell_type":"code","source":"NUMBER = 152\n\ntest_path = '../input/datab/dataB/CameraRGB/'\ntest_list = os.listdir(test_path)\ntest_list = [test_path+i for i in test_list]\ntest = imageio.imread(test_list[NUMBER])\n\nplt.imshow(test)\n#my_preds = model.predict(np.expand_dims(test, 0))\n#my_preds.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict_generator(test_generator,steps = STEP_SIZE_TEST)\n#predictions = model_new.predict_generator(test_generator,steps = STEP_SIZE_TEST)\n\nnp.shape(predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Vizualizing predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"#single_layer = np.argmax(predictions[0], axis=-1)\n#print(np.shape(single_layer))\n#output = np.zeros(predictions.shape[1:3]+(1,) )\n#for k in range(13):\n#    output[single_layer==k] = k\n#np.shape(output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def onehot_to_rgb(onehot):\n    '''Function to decode encoded mask labels\n        Inputs: \n            onehot - one hot encoded image matrix (height x width x num_classes)\n            colormap - dictionary of color to label id\n        Output: Decoded RGB image (height x width x 3) \n    '''\n    single_layer = np.argmax(onehot, axis=-1)\n    #print(np.shape(single_layer))\n    output = np.zeros((600, 800, 1))\n    for k in range(13):\n        output[single_layer==k] = k\n    #np.shape(output)\n    return np.uint8(output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.shape(X_test[0]), np.shape(test_masks[0]), np.shape(onehot_to_rgb(predictions[0])), np.shape(output)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0,np.shape(predictions)[0]):\n#for i in range(5):\n    \n    fig = plt.figure(figsize=(20,8))\n    \n    ax1 = fig.add_subplot(1,3,1)\n    ax1.imshow(X_test[i])\n    ax1.title.set_text('Actual frame')\n    ax1.grid(b=None)\n    \n    \n    ax2 = fig.add_subplot(1,3,2)\n    ax2.set_title('Ground truth labels')\n    ax2.imshow(np.squeeze(test_masks[i]))\n    ax2.grid(b=None)\n    \n    ax3 = fig.add_subplot(1,3,3)\n    ax3.set_title('Predicted labels')\n    ax3.imshow(np.squeeze(onehot_to_rgb(predictions[i])))\n    ax3.grid(b=None)\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Applitcation aux images sat:\n\n1. Comment conserver les positions géographiques des pixels\n2. Comment gérer les GeoTiff"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Infering"},{"metadata":{"trusted":true},"cell_type":"code","source":"def infering(array):\n    new = np.zeros((len(array),), dtype=np.uint8)\n    index = 0\n    \n    for i in array:\n        \n        if i < 0.5:\n            new[index] = 0\n        elif i >= 0.5 and i < 1.5:\n            new[index] = 1\n        elif i >= 1.5 and i < 2.5:\n            new[index] = 2\n        elif i < 3.5 and i >= 2.5:\n            new[index] = 3\n        elif i < 4.5 and i >= 3.5:\n            new[index] = 4\n        elif i < 5.5 and i >= 4.5:\n            new[index] = 5\n        elif i < 6.5 and i >= 5.5:\n            new[index] = 6\n        elif i < 7.5 and i >= 6.5:\n            new[index] = 7\n        elif i < 8.5 and i >= 7.5:\n            new[index] = 8\n        elif i < 9.5 and i >= 8.5:\n            new[index] = 9\n        elif i < 10.5 and i >= 9.5:\n            new[index] = 10\n        elif i < 11.5 and i >= 10.5:\n            new[index] = 11\n        elif i < 12.5 and i >= 11.5:\n            new[index] = 12\n        index += 1\n    return new","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}